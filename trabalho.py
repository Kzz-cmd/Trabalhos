# -*- coding: utf-8 -*-
"""TRABALHO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-HS53XsIKfdDoS2U98R7RyS_oSg__qS
"""

import gymnasium as gym
from stable_baselines3 import DQN
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, confusion_matrix, f1_score

# 1Ô∏è‚É£ Criar o ambiente
env = gym.make("Taxi-v3", render_mode="ansi")

# 2Ô∏è‚É£ Coletar dataset de experi√™ncias aleat√≥rias
dataset = []
state, info = env.reset()
state = int(state)
done = False

while not done:
    action = env.action_space.sample()
    next_state, reward, terminated, truncated, info = env.step(action)
    next_state = int(next_state)

    dataset.append((state, action, reward, next_state, terminated))
    state = next_state
    done = terminated or truncated

print(f"Dataset coletado com {len(dataset)} experi√™ncias.\n")

# 3Ô∏è‚É£ Treinar agente RL usando DQN
model = DQN("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)

# 4Ô∏è‚É£ Testar agente treinado e coletar m√©tricas
num_episodes = 5
all_rewards = []
all_steps = []

# Para m√©tricas de predi√ß√£o de a√ß√µes
y_true = []  # a√ß√µes do dataset
y_pred = []  # a√ß√µes do agente

print("üöñ Testando agente treinado...\n")

for ep in range(num_episodes):
    state, _ = env.reset()
    state = int(state)
    done = False
    total_reward = 0
    steps = 0

    while not done:
        # Escolher a√ß√£o do agente
        action, _ = model.predict(state)
        next_state, reward, terminated, truncated, info = env.step(int(action))

        # Para m√©tricas
        # Como n√£o temos "√≥timo", vamos comparar com uma a√ß√£o aleat√≥ria do dataset
        y_true.append(np.random.choice([a for s, a, r, ns, t in dataset]))
        y_pred.append(int(action))

        state = int(next_state)
        total_reward += reward
        steps += 1
        done = terminated or truncated

    all_rewards.append(total_reward)
    all_steps.append(steps)
    print(f"‚úÖ Epis√≥dio {ep+1} finalizado: Recompensa = {total_reward}, Passos = {steps}")

# 5Ô∏è‚É£ Estat√≠sticas gerais
print("\nüìä M√©tricas gerais do teste:")
print(f"M√©dia de recompensas: {np.mean(all_rewards):.2f}")
print(f"Desvio padr√£o das recompensas: {np.std(all_rewards):.2f}")
print(f"M√©dia de passos por epis√≥dio: {np.mean(all_steps):.2f}")
print(f"Desvio padr√£o dos passos: {np.std(all_steps):.2f}")

# 6Ô∏è‚É£ M√©tricas de predi√ß√£o de a√ß√£o
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
cm = confusion_matrix(y_true, y_pred)
f1 = f1_score(y_true, y_pred, average='macro')  # m√©dia macro para v√°rias classes

print("\nüìä M√©tricas de predi√ß√£o de a√ß√£o:")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"F1-score (macro): {f1:.2f}")
print(f"Matriz de confus√£o:\n{cm}")

env.close()
print("‚úÖ Teste finalizado.")